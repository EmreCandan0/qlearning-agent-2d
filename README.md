# ğŸ§  Q-Learning Agent in a 2D Grid World

This project demonstrates a simple **Q-learning agent** navigating through a 2D grid environment. The agent learns â€” through trial and error â€” how to reach a goal position while avoiding obstacles.

---

## ğŸŒŸ Features

- 2D grid environment (default: 5x5)
- Customizable start, goal, and wall positions
- Reward structure: +10 for goal, -1 for every move
- Q-learning algorithm implementation
- Console visualization of the learned path
- Modular and beginner-friendly code structure

---

## ğŸ§  Q-Learning Formula

The agent learns by updating a Q-table using the following formula:
Q(s, a) â† Q(s, a) + Î± Ã— (r + Î³ Ã— max Q(sâ€™, aâ€™) âˆ’ Q(s, a))


Where:
- `s` is the current state (position)
- `a` is the action taken
- `r` is the immediate reward
- `sâ€™` is the new state after action
- `Î±` is the learning rate
- `Î³` is the discount factor

---

## ğŸ“‚ Project Structure

| File            | Description                              |
|-----------------|------------------------------------------|
| `environment.py`| The 2D grid world and step logic         |
| `agent.py`      | Q-learning agent logic                   |
| `train.py`      | Trains the agent in the environment      |
| `visualize.py`  | Displays the path learned by the agent   |
| `README.md`     | Project documentation                    |

---

## ğŸ›  How to Run

1. Clone the repository:
git clone https://github.com/yourusername/qlearning-agent-2d.git
cd qlearning-agent-2d

2.Train the agent:
python3 train.py

3.Visualize the learned path:
python3 visualize.py

ğŸ–¼ Sample Output (Terminal Visualization)

S ğŸŸ© â¬œ â¬œ â¬œ
â¬œ â¬› â¬œ â¬œ â¬œ
â¬œ â¬œ â¬› â¬œ â¬œ
â¬œ â¬œ â¬œ â¬› â¬œ
â¬œ â¬œ â¬œ â¬œ G

S â†’ Start

G â†’ Goal

â¬› â†’ Wall

ğŸŸ© â†’ Agent's path

â¬œ â†’ Empty cell





